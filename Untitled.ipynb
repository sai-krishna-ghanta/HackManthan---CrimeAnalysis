{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb082e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-18 14:39:35.897 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\sanke\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `get_data()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function get_data at 0x000002375F7F2B80>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;31m# Hash the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"%s:%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    409\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[1;32m--> 410\u001b[1;33m             \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[1;31m# script path in ScriptRunner.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mmain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39892/676848175.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconfirmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeaths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mconfirmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0mFIPSs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfirmed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Province_State'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Admin2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFIPS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mFIPSs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'State'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'County'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'FIPS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\caching.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshow_spinner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspinner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mget_or_create_cached_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_or_create_cached_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\caching.py\u001b[0m in \u001b[0;36mget_or_create_cached_value\u001b[1;34m()\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[1;31m# If we generated the key earlier we would only hash those\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m                 \u001b[1;31m# globals by name, and miss changes in their code or value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m                 \u001b[0mcache_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_hash_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_funcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[1;31m# First, get the cache that's attached to this function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\caching.py\u001b[0m in \u001b[0;36m_hash_func\u001b[1;34m(func, hash_funcs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;31m# because this step will be hashing any objects referenced in the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;31m# body.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m     update_hash(\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc_hasher\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CodeHasher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_funcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, hasher, obj, context)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mContext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[1;34m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mInternalHashError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;31m# Hash the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"%s:%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;31m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__code__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m                 \u001b[0mdefaults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__defaults__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[1;32m--> 410\u001b[1;33m             \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;31m# This works because we set __main__.__file__ to the report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[1;31m# script path in ScriptRunner.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mmain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `get_data()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function get_data at 0x000002375F7F2B80>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import date\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "_ENABLE_PROFILING = False\n",
    "\n",
    "if _ENABLE_PROFILING:\n",
    "    import cProfile, pstats, io\n",
    "    from pstats import SortKey\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"COVID19: EpiCenter for Disease Dynamics\",\n",
    "    layout='wide',\n",
    "    initial_sidebar_state='auto',\n",
    ")\n",
    "\n",
    "sidebar_selection = st.sidebar.radio(\n",
    "    'Select data:',\n",
    "    ['Select Counties', 'California'],\n",
    ")\n",
    "\n",
    "@st.cache(ttl=3*60*60, suppress_st_warning=True)\n",
    "def get_data():\n",
    "    US_confirmed = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "    US_deaths = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "    confirmed = pd.read_csv(US_confirmed)\n",
    "    deaths = pd.read_csv(US_deaths)\n",
    "    return confirmed, deaths\n",
    "\n",
    "confirmed, deaths = get_data()\n",
    "FIPSs = confirmed.groupby(['Province_State', 'Admin2']).FIPS.unique().apply(pd.Series).reset_index()\n",
    "FIPSs.columns = ['State', 'County', 'FIPS']\n",
    "FIPSs['FIPS'].fillna(0, inplace = True)\n",
    "FIPSs['FIPS'] = FIPSs.FIPS.astype(int).astype(str).str.zfill(5)\n",
    "\n",
    "@st.cache(ttl=3*60*60, suppress_st_warning=True)\n",
    "def get_testing_data(County):\n",
    "    apiKey = '9fe19182c5bf4d1bb105da08e593a578'\n",
    "    if len(County) == 1:\n",
    "        #print(len(County))\n",
    "        f = FIPSs[FIPSs.County == County[0]].FIPS.values[0]\n",
    "        #print(f)\n",
    "        path1 = 'https://data.covidactnow.org/latest/us/counties/'+f+'.OBSERVED_INTERVENTION.timeseries.json?apiKey='+apiKey\n",
    "        #print(path1)\n",
    "        df = json.loads(requests.get(path1).text)\n",
    "        #print(df.keys())\n",
    "        data = pd.DataFrame.from_dict(df['actualsTimeseries'])\n",
    "        data['Date'] = pd.to_datetime(data['date'])\n",
    "        data = data.set_index('Date')\n",
    "        #print(data.tail())\n",
    "        try:\n",
    "            data['new_negative_tests'] = data['cumulativeNegativeTests'].diff()\n",
    "            data.loc[(data['new_negative_tests'] < 0)] = np.nan\n",
    "        except: \n",
    "            data['new_negative_tests'] = np.nan\n",
    "            st.text('Negative test data not avilable')\n",
    "        data['new_negative_tests_rolling'] = data['new_negative_tests'].fillna(0).rolling(14).mean()\n",
    "\n",
    "\n",
    "        try:\n",
    "            data['new_positive_tests'] = data['cumulativePositiveTests'].diff()\n",
    "            data.loc[(data['new_positive_tests'] < 0)] = np.nan\n",
    "        except: \n",
    "            data['new_positive_tests'] = np.nan\n",
    "            st.text('test data not avilable')\n",
    "        data['new_positive_tests_rolling'] = data['new_positive_tests'].fillna(0).rolling(14).mean()\n",
    "        data['new_tests'] = data['new_negative_tests']+data['new_positive_tests']\n",
    "        data['new_tests_rolling'] = data['new_tests'].fillna(0).rolling(14).mean()\n",
    "        data['testing_positivity_rolling'] = (data['new_positive_tests_rolling'] / data['new_tests_rolling'])*100\n",
    "        #data['testing_positivity_rolling'].tail(14).plot()\n",
    "        #plt.show()\n",
    "        return data['new_tests_rolling'], data['testing_positivity_rolling'].iloc[-1:].values[0]\n",
    "    elif (len(County) > 1) & (len(County) < 5):\n",
    "        new_positive_tests = []\n",
    "        new_negative_tests = []\n",
    "        new_tests = []\n",
    "        for c in County:\n",
    "            f = FIPSs[FIPSs.County == c].FIPS.values[0]\n",
    "            path1 = 'https://data.covidactnow.org/latest/us/counties/'+f+'.OBSERVED_INTERVENTION.timeseries.json?apiKey='+apiKey\n",
    "            df = json.loads(requests.get(path1).text)\n",
    "            data = pd.DataFrame.from_dict(df['actualsTimeseries'])\n",
    "            data['Date'] = pd.to_datetime(data['date'])\n",
    "            data = data.set_index('Date')\n",
    "            try:\n",
    "                data['new_negative_tests'] = data['cumulativeNegativeTests'].diff()\n",
    "                data.loc[(data['new_negative_tests'] < 0)] = np.nan\n",
    "            except: \n",
    "                data['new_negative_tests'] = np.nan\n",
    "                #print('Negative test data not avilable')\n",
    "\n",
    "            try:\n",
    "                data['new_positive_tests'] = data['cumulativePositiveTests'].diff()\n",
    "                data.loc[(data['new_positive_tests'] < 0)] = np.nan\n",
    "            except: \n",
    "                data['new_positive_tests'] = np.nan\n",
    "                #print('Negative test data not avilable')\n",
    "            data['new_tests'] = data['new_negative_tests']+data['new_positive_tests']\n",
    "\n",
    "            new_positive_tests.append(data['new_positive_tests'])\n",
    "            #new_negative_tests.append(data['new_tests'])\n",
    "            new_tests.append(data['new_tests'])\n",
    "            #print(data.head())\n",
    "\n",
    "        new_positive_tests_rolling = pd.concat(new_positive_tests, axis = 1).sum(axis = 1)\n",
    "        new_positive_tests_rolling = new_positive_tests_rolling.fillna(0).rolling(14).mean()\n",
    "        #print('new test merging of counties')\n",
    "        #print(pd.concat(new_tests, axis = 1).head().sum(axis = 1))\n",
    "        new_tests_rolling = pd.concat(new_tests, axis = 1).sum(axis = 1)\n",
    "        new_tests_rolling = new_tests_rolling.fillna(0).rolling(14).mean()\n",
    "        new_tests_rolling = pd.DataFrame(new_tests_rolling).fillna(0)\n",
    "        new_tests_rolling.columns = ['new_tests_rolling']\n",
    "        #print('whole df')\n",
    "        #print(type(new_tests_rolling))\n",
    "        #print(new_tests_rolling.head())\n",
    "        #print('single column')\n",
    "        #print(new_tests_rolling['new_tests_rolling'].head())\n",
    "        #print('new_positive_tests_rolling')\n",
    "        #print(new_positive_tests_rolling.head())\n",
    "        #print('new_tests_rolling')\n",
    "        #print(new_tests_rolling.head())\n",
    "        data_to_show = (new_positive_tests_rolling / new_tests_rolling.new_tests_rolling)*100\n",
    "        #print(data_to_show.shape)\n",
    "        #print(data_to_show.head())\n",
    "        #print(data_to_show.columns)\n",
    "        #print(data_to_show.iloc[-1:].values[0])\n",
    "        return new_tests_rolling, data_to_show.iloc[-1:].values[0]\n",
    "    else:\n",
    "        st.text('Getting testing data for California State')\n",
    "        path1 = 'https://data.covidactnow.org/latest/us/states/CA.OBSERVED_INTERVENTION.timeseries.json'\n",
    "        df = json.loads(requests.get(path1).text)\n",
    "        data = pd.DataFrame.from_dict(df['actualsTimeseries'])\n",
    "        data['Date'] = pd.to_datetime(data['date'])\n",
    "        data = data.set_index('Date')\n",
    "\n",
    "        try:\n",
    "            data['new_negative_tests'] = data['cumulativeNegativeTests'].diff()\n",
    "            data.loc[(data['new_negative_tests'] < 0)] = np.nan\n",
    "        except:\n",
    "            data['new_negative_tests'] = np.nan\n",
    "            print('Negative test data not available')\n",
    "        data['new_negative_tests_rolling'] = data['new_negative_tests'].fillna(0).rolling(14).mean()\n",
    "\n",
    "\n",
    "        try:\n",
    "            data['new_positive_tests'] = data['cumulativePositiveTests'].diff()\n",
    "            data.loc[(data['new_positive_tests'] < 0)] = np.nan\n",
    "        except:\n",
    "            data['new_positive_tests'] = np.nan\n",
    "            st.text('test data not available')\n",
    "        data['new_positive_tests_rolling'] = data['new_positive_tests'].fillna(0).rolling(14).mean()\n",
    "        data['new_tests'] = data['new_negative_tests']+data['new_positive_tests']\n",
    "        data['new_tests_rolling'] = data['new_tests'].fillna(0).rolling(14).mean()\n",
    "        data['testing_positivity_rolling'] = (data['new_positive_tests_rolling'] / data['new_tests_rolling'])*100\n",
    "        return data['new_tests_rolling'], data['testing_positivity_rolling'].iloc[-1:].values[0]\n",
    "\n",
    "\n",
    "def plot_county(county):\n",
    "    testing_df, testing_percent = get_testing_data(County=county)\n",
    "    #print(testing_df.head())\n",
    "    county_confirmed = confirmed[confirmed.Admin2.isin(county)]\n",
    "    county_confirmed_time = county_confirmed.drop(county_confirmed.iloc[:, 0:12], axis=1).T\n",
    "    county_confirmed_time = county_confirmed_time.sum(axis= 1)\n",
    "    county_confirmed_time = county_confirmed_time.reset_index()\n",
    "    county_confirmed_time.columns = ['date', 'cases']\n",
    "    county_confirmed_time['Datetime'] = pd.to_datetime(county_confirmed_time['date'])\n",
    "    county_confirmed_time = county_confirmed_time.set_index('Datetime')\n",
    "    del county_confirmed_time['date']\n",
    "    incidence= pd.DataFrame(county_confirmed_time.cases.diff())\n",
    "    incidence.columns = ['incidence']\n",
    "    chart_max = incidence.max().values[0]+500\n",
    "\n",
    "    county_deaths = deaths[deaths.Admin2.isin(county)]\n",
    "    population = county_deaths.Population.values.sum()\n",
    "\n",
    "    del county_deaths['Population']\n",
    "    county_deaths_time = county_deaths.drop(county_deaths.iloc[:, 0:11], axis=1).T\n",
    "    county_deaths_time = county_deaths_time.sum(axis= 1)\n",
    "\n",
    "    county_deaths_time = county_deaths_time.reset_index()\n",
    "    county_deaths_time.columns = ['date', 'deaths']\n",
    "    county_deaths_time['Datetime'] = pd.to_datetime(county_deaths_time['date'])\n",
    "    county_deaths_time = county_deaths_time.set_index('Datetime')\n",
    "    del county_deaths_time['date']\n",
    "\n",
    "    cases_per100k  = ((county_confirmed_time) * 100000 / population)\n",
    "    cases_per100k.columns = ['cases per 100K']\n",
    "    cases_per100k['rolling average'] = cases_per100k['cases per 100K'].rolling(7).mean()\n",
    "\n",
    "    deaths_per100k  = ((county_deaths_time) * 100000 / population)\n",
    "    deaths_per100k.columns = ['deaths per 100K']\n",
    "    deaths_per100k['rolling average'] = deaths_per100k['deaths per 100K'].rolling(7).mean()\n",
    "\n",
    "\n",
    "    incidence['rolling_incidence'] = incidence.incidence.rolling(7).mean()\n",
    "    metric = (incidence['rolling_incidence'] * 100000 / population).iloc[[-1]]\n",
    "\n",
    "    if len(county) == 1:\n",
    "        st.subheader('Current situation of COVID-19 cases in '+', '.join(map(str, county))+' county ('+ str(today)+')')\n",
    "    else:\n",
    "        st.subheader('Current situation of COVID-19 cases in '+', '.join(map(str, county))+' counties ('+ str(today)+')')\n",
    "\n",
    "    c1 = st.container()\n",
    "    c2 = st.container()\n",
    "    c3 = st.container()\n",
    "\n",
    "    if len(county)==1:\n",
    "        C = county[0]\n",
    "        with c2:\n",
    "            a1, _, a2 = st.columns((3.9, 0.2, 3.9))     \n",
    "            with a1:\n",
    "                f = FIPSs[FIPSs.County == C].FIPS.values[0]\n",
    "                components.iframe(\"https://covidactnow.org/embed/us/county/\"+f, width=350, height=365, scrolling=False)\n",
    "                \n",
    "            with a2:\n",
    "                st.markdown('New cases averaged over last 7 days = %s' %'{:,.1f}'.format(metric.values[0]))\n",
    "                st.markdown(\"Population under consideration = %s\"% '{:,.0f}'.format(population))\n",
    "                st.markdown(\"Total cases = %s\"% '{:,.0f}'.format(county_confirmed_time.tail(1).values[0][0]))\n",
    "                st.markdown(\"Total deaths = %s\"% '{:,.0f}'.format(county_deaths_time.tail(1).values[0][0]))\n",
    "                st.markdown(\"% test positivity (14 day average)* = \"+\"%.2f\" % testing_percent)\n",
    "    elif len(county) <= 3:\n",
    "        with c2:\n",
    "            st.write('')\n",
    "            st.write('')\n",
    "            st.markdown(\"New cases averaged over last 7 days = %s\" % \"{:,.1f}\".format(metric.values[0]))\n",
    "            st.markdown(\"Population under consideration = %s\"% '{:,.0f}'.format(population))\n",
    "            st.markdown(\"Total cases = %s\"% '{:,.0f}'.format(county_confirmed_time.tail(1).values[0][0]))\n",
    "            st.markdown(\"Total deaths = %s\"% '{:,.0f}'.format(county_deaths_time.tail(1).values[0][0]))\n",
    "            st.markdown(\"% test positivity (14 day average)* = \"+\"%.2f\" % testing_percent)\n",
    "        with c3:\n",
    "            columns = st.columns(len(county))\n",
    "            for idx, C in enumerate(county):\n",
    "                with columns[idx]:\n",
    "                    st.write('')\n",
    "                    st.write('')\n",
    "                    f = FIPSs[FIPSs.County == C].FIPS.values[0]\n",
    "                    components.iframe(\"https://covidactnow.org/embed/us/county/\"+f, width=350, height=365, scrolling=False)\n",
    "\n",
    "    ### Experiment with Altair instead of Matplotlib.\n",
    "    with c1:\n",
    "        a2, _, a1 = st.columns((3.9, 0.2, 3.9))\n",
    "\n",
    "        incidence = incidence.reset_index()\n",
    "        incidence['nomalized_rolling_incidence'] = incidence['rolling_incidence'] * 100000 / population\n",
    "        incidence['Phase 2 Threshold'] = 25\n",
    "        incidence['Phase 3 Threshold'] = 10\n",
    "        scale = alt.Scale(\n",
    "            domain=[\n",
    "                \"rolling_incidence\",\n",
    "                \"Phase 2 Threshold\",\n",
    "                \"Phase 3 Threshold\"\n",
    "            ], range=['#377eb8', '#e41a1c', '#4daf4a'])\n",
    "        base = alt.Chart(\n",
    "            incidence,\n",
    "            title='(A) Weekly rolling mean of incidence per 100K'\n",
    "        ).transform_calculate(\n",
    "            base_=\"'rolling_incidence'\",\n",
    "            phase2_=\"'Phase 2 Threshold'\",\n",
    "            phase3_=\"'Phase 3 Threshold'\",\n",
    "        )\n",
    "        \n",
    "        ax4 = base.mark_line(strokeWidth=3).encode(\n",
    "            x=alt.X(\"Datetime\", axis = alt.Axis(title='Date')),\n",
    "            y=alt.Y(\"nomalized_rolling_incidence\", axis=alt.Axis(title='per 100 thousand')),\n",
    "            color=alt.Color(\"base_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "\n",
    "        line1 = base.mark_line(strokeDash=[8, 8], strokeWidth=2).encode(\n",
    "            x=alt.X(\"Datetime\", axis=alt.Axis(title = 'Date')),\n",
    "            y=alt.Y(\"Phase 2 Threshold\", axis=alt.Axis(title='Count')),\n",
    "            color=alt.Color(\"phase2_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "\n",
    "        line2 = base.mark_line(strokeDash=[8, 8], strokeWidth=2).encode(\n",
    "            x=alt.X(\"Datetime\", axis=alt.Axis(title='Date')),\n",
    "            y=alt.Y(\"Phase 3 Threshold\", axis=alt.Axis(title='Count')),\n",
    "            color=alt.Color(\"phase3_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "\n",
    "        with a2:\n",
    "            st.altair_chart(ax4 + line1 + line2, use_container_width=True)\n",
    "\n",
    "        ax3 = alt.Chart(incidence, title = '(B) Daily incidence (new cases)').mark_bar().encode(\n",
    "            x=alt.X(\"Datetime\",axis = alt.Axis(title = 'Date')),\n",
    "            y=alt.Y(\"incidence\",axis = alt.Axis(title = 'Incidence'), scale=alt.Scale(domain=(0, chart_max), clamp=True))\n",
    "        )\n",
    "        \n",
    "        with a1:\n",
    "            st.altair_chart(ax3, use_container_width=True)\n",
    "        \n",
    "        a3, _, a4 = st.columns((3.9, 0.2, 3.9))\n",
    "        testing_df = pd.DataFrame(testing_df).reset_index()\n",
    "        #print(testing_df.head())\n",
    "        #print(type(testing_df))\n",
    "        \n",
    "        base = alt.Chart(testing_df, title = '(D) Daily new tests').mark_line(strokeWidth=3).encode(\n",
    "            x=alt.X(\"Date\",axis = alt.Axis(title = 'Date')),\n",
    "            y=alt.Y(\"new_tests_rolling\",axis = alt.Axis(title = 'Daily new tests'))\n",
    "        )\n",
    "        with a4:\n",
    "            st.altair_chart(base, use_container_width=True)\n",
    "\n",
    "        county_confirmed_time = county_confirmed_time.reset_index()\n",
    "        county_deaths_time = county_deaths_time.reset_index()\n",
    "        cases_and_deaths = county_confirmed_time.set_index(\"Datetime\").join(county_deaths_time.set_index(\"Datetime\"))\n",
    "        cases_and_deaths = cases_and_deaths.reset_index()\n",
    "\n",
    "        # Custom colors for layered charts.\n",
    "        # See https://stackoverflow.com/questions/61543503/add-legend-to-line-bars-to-altair-chart-without-using-size-color.\n",
    "        scale = alt.Scale(domain=[\"cases\", \"deaths\"], range=['#377eb8', '#e41a1c'])\n",
    "        base = alt.Chart(\n",
    "            cases_and_deaths,\n",
    "            title='(C) Cumulative cases and deaths'\n",
    "        ).transform_calculate(\n",
    "            cases_=\"'cases'\",\n",
    "            deaths_=\"'deaths'\",\n",
    "        )\n",
    "\n",
    "        c = base.mark_line(strokeWidth=3).encode(\n",
    "            x=alt.X(\"Datetime\", axis=alt.Axis(title = 'Date')),\n",
    "            y=alt.Y(\"cases\", axis=alt.Axis(title = 'Count')),\n",
    "            color=alt.Color(\"cases_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "\n",
    "        d = base.mark_line(strokeWidth=3).encode(\n",
    "            x=alt.X(\"Datetime\", axis=alt.Axis(title='Date')),\n",
    "            y=alt.Y(\"deaths\", axis=alt.Axis(title = 'Count')),\n",
    "            color=alt.Color(\"deaths_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "        with a3:\n",
    "            st.altair_chart(c+d, use_container_width=True)\n",
    "\n",
    "\n",
    "def plot_state():\n",
    "    @st.cache(ttl=3*60*60, suppress_st_warning=True)\n",
    "    def get_testing_data_state():\n",
    "            st.text('Getting testing data for California State')\n",
    "            path1 = 'https://data.covidactnow.org/latest/us/states/CA.OBSERVED_INTERVENTION.timeseries.json'\n",
    "            df = json.loads(requests.get(path1).text)\n",
    "            data = pd.DataFrame.from_dict(df['actualsTimeseries'])\n",
    "            data['Date'] = pd.to_datetime(data['date'])\n",
    "            data = data.set_index('Date')\n",
    "\n",
    "            try:\n",
    "                data['new_negative_tests'] = data['cumulativeNegativeTests'].diff()\n",
    "                data.loc[(data['new_negative_tests'] < 0)] = np.nan\n",
    "            except:\n",
    "                data['new_negative_tests'] = np.nan\n",
    "                print('Negative test data not available')\n",
    "            data['new_negative_tests_rolling'] = data['new_negative_tests'].fillna(0).rolling(14).mean()\n",
    "\n",
    "\n",
    "            try:\n",
    "                data['new_positive_tests'] = data['cumulativePositiveTests'].diff()\n",
    "                data.loc[(data['new_positive_tests'] < 0)] = np.nan\n",
    "            except:\n",
    "                data['new_positive_tests'] = np.nan\n",
    "                st.text('test data not available')\n",
    "            data['new_positive_tests_rolling'] = data['new_positive_tests'].fillna(0).rolling(14).mean()\n",
    "            data['new_tests'] = data['new_negative_tests']+data['new_positive_tests']\n",
    "            data['new_tests_rolling'] = data['new_tests'].fillna(0).rolling(14).mean()\n",
    "            data['testing_positivity_rolling'] = (data['new_positive_tests_rolling'] / data['new_tests_rolling'])*100\n",
    "            # return data['new_tests_rolling'], data['testing_positivity_rolling'].iloc[-1:].values[0]\n",
    "            testing_df, testing_percent = data['new_tests_rolling'], data['testing_positivity_rolling'].iloc[-1:].values[0]\n",
    "            county_confirmed = confirmed[confirmed.Province_State == 'California']\n",
    "            #county_confirmed = confirmed[confirmed.Admin2 == county]\n",
    "            county_confirmed_time = county_confirmed.drop(county_confirmed.iloc[:, 0:12], axis=1).T #inplace=True, axis=1\n",
    "            county_confirmed_time = county_confirmed_time.sum(axis= 1)\n",
    "            county_confirmed_time = county_confirmed_time.reset_index()\n",
    "            county_confirmed_time.columns = ['date', 'cases']\n",
    "            county_confirmed_time['Datetime'] = pd.to_datetime(county_confirmed_time['date'])\n",
    "            county_confirmed_time = county_confirmed_time.set_index('Datetime')\n",
    "            del county_confirmed_time['date']\n",
    "            #print(county_confirmed_time.head())\n",
    "            incidence = pd.DataFrame(county_confirmed_time.cases.diff())\n",
    "            incidence.columns = ['incidence']\n",
    "\n",
    "            #temp_df_time = temp_df.drop(['date'], axis=0).T #inplace=True, axis=1\n",
    "            county_deaths = deaths[deaths.Province_State == 'California']\n",
    "            population = county_deaths.Population.values.sum()\n",
    "\n",
    "            del county_deaths['Population']\n",
    "            county_deaths_time = county_deaths.drop(county_deaths.iloc[:, 0:11], axis=1).T #inplace=True, axis=1\n",
    "            county_deaths_time = county_deaths_time.sum(axis= 1)\n",
    "\n",
    "            county_deaths_time = county_deaths_time.reset_index()\n",
    "            county_deaths_time.columns = ['date', 'deaths']\n",
    "            county_deaths_time['Datetime'] = pd.to_datetime(county_deaths_time['date'])\n",
    "            county_deaths_time = county_deaths_time.set_index('Datetime')\n",
    "            del county_deaths_time['date']\n",
    "\n",
    "            cases_per100k  = ((county_confirmed_time)*100000/population)\n",
    "            cases_per100k.columns = ['cases per 100K']\n",
    "            cases_per100k['rolling average'] = cases_per100k['cases per 100K'].rolling(7).mean()\n",
    "\n",
    "            deaths_per100k  = ((county_deaths_time)*100000/population)\n",
    "            deaths_per100k.columns = ['deaths per 100K']\n",
    "            deaths_per100k['rolling average'] = deaths_per100k['deaths per 100K'].rolling(7).mean()\n",
    "\n",
    "            incidence['rolling_incidence'] = incidence.incidence.rolling(7).mean()\n",
    "            return population, testing_df, testing_percent, county_deaths_time, county_confirmed_time, incidence\n",
    "    # metric = (incidence['rolling_incidence']*100000/population).iloc[[-1]]\n",
    "\n",
    "    #print(county_deaths_time.tail(1).values[0])\n",
    "    #print(cases_per100k.head())\n",
    "    population, testing_df, testing_percent, county_deaths_time, county_confirmed_time, incidence = get_testing_data_state()\n",
    "    st.subheader('Current situation of COVID-19 cases in California ('+ str(today)+')')\n",
    "    c1 = st.container()\n",
    "    c2 = st.container()\n",
    "    c3 = st.container()\n",
    "\n",
    "    with c2:\n",
    "        a1, _, a2 = st.columns((3.9, 0.2, 3.9))     \n",
    "        with a1:\n",
    "            #f = FIPSs[FIPSs.County == C].FIPS.values[0]\n",
    "            components.iframe(\"https://covidactnow.org/embed/us/california-ca\", width=350, height=365, scrolling=False)\n",
    "\n",
    "        with a2:\n",
    "            st.markdown(\"Population under consideration = %s\"% '{:,.0f}'.format(population))\n",
    "            st.markdown(\"% test positivity (14 day average) = \"+\"%.2f\" % testing_percent)\n",
    "            st.markdown(\"Total cases = %s\"% '{:,.0f}'.format(county_confirmed_time.tail(1).values[0][0]))\n",
    "            st.markdown(\"Total deaths = %s\"% '{:,.0f}'.format(county_deaths_time.tail(1).values[0][0]))\n",
    "            \n",
    "    ### Experiment with Altair instead of Matplotlib.\n",
    "    with c1:\n",
    "        a2, _, a1 = st.columns((3.9, 0.2, 3.9))\n",
    "\n",
    "        incidence = incidence.reset_index()\n",
    "        incidence['nomalized_rolling_incidence'] = incidence['rolling_incidence'] * 100000 / population\n",
    "        incidence['Phase 2 Threshold'] = 25\n",
    "        incidence['Phase 3 Threshold'] = 10\n",
    "        \n",
    "        scale = alt.Scale(\n",
    "            domain=[\n",
    "                \"rolling_incidence\",\n",
    "                \"Phase 2 Threshold\",\n",
    "                \"Phase 3 Threshold\"\n",
    "            ], range=['#377eb8', '#e41a1c', '#4daf4a'])\n",
    "        base = alt.Chart(\n",
    "            incidence,\n",
    "            title='(A) Weekly rolling mean of incidence per 100K'\n",
    "        ).transform_calculate(\n",
    "            base_=\"'rolling_incidence'\",\n",
    "            phase2_=\"'Phase 2 Threshold'\",\n",
    "            phase3_=\"'Phase 3 Threshold'\",\n",
    "        )\n",
    "        \n",
    "        ax4 = base.mark_line(strokeWidth=3).encode(\n",
    "            x=alt.X(\"Datetime\", axis = alt.Axis(title='Date')),\n",
    "            y=alt.Y(\"nomalized_rolling_incidence\", axis=alt.Axis(title='per 100 thousand')),\n",
    "            color=alt.Color(\"base_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "\n",
    "        line1 = base.mark_line(strokeDash=[8, 8], strokeWidth=2).encode(\n",
    "            x=alt.X(\"Datetime\", axis=alt.Axis(title = 'Date')),\n",
    "            y=alt.Y(\"Phase 2 Threshold\", axis=alt.Axis(title='Count')),\n",
    "            color=alt.Color(\"phase2_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "\n",
    "        line2 = base.mark_line(strokeDash=[8, 8], strokeWidth=2).encode(\n",
    "            x=alt.X(\"Datetime\", axis=alt.Axis(title='Date')),\n",
    "            y=alt.Y(\"Phase 3 Threshold\", axis=alt.Axis(title='Count')),\n",
    "            color=alt.Color(\"phase3_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "        with a2:\n",
    "            st.altair_chart(ax4 + line1 + line2, use_container_width=True)\n",
    "\n",
    "        ax3 = alt.Chart(incidence, title = '(B) Daily incidence (new cases)').mark_bar().encode(\n",
    "            x=alt.X(\"Datetime\",axis = alt.Axis(title = 'Date')),\n",
    "            y=alt.Y(\"incidence\",axis = alt.Axis(title = 'Incidence'))\n",
    "        )\n",
    "        \n",
    "        with a1:\n",
    "            st.altair_chart(ax3, use_container_width=True)\n",
    "        \n",
    "        a3, _, a4 = st.columns((3.9, 0.2, 3.9))\n",
    "        testing_df = pd.DataFrame(testing_df).reset_index()\n",
    "        #print(testing_df.head())\n",
    "        #print(type(testing_df))\n",
    "        \n",
    "        base = alt.Chart(testing_df, title = '(D) Daily new tests').mark_line(strokeWidth=3).encode(\n",
    "            x=alt.X(\"Date\",axis = alt.Axis(title = 'Date')),\n",
    "            y=alt.Y(\"new_tests_rolling\",axis = alt.Axis(title = 'Daily new tests'))\n",
    "        )\n",
    "        with a4:\n",
    "            st.altair_chart(base, use_container_width=True)\n",
    "\n",
    "        county_confirmed_time = county_confirmed_time.reset_index()\n",
    "        county_deaths_time = county_deaths_time.reset_index()\n",
    "        cases_and_deaths = county_confirmed_time.set_index(\"Datetime\").join(county_deaths_time.set_index(\"Datetime\"))\n",
    "        cases_and_deaths = cases_and_deaths.reset_index()\n",
    "\n",
    "        # Custom colors for layered charts.\n",
    "        # See https://stackoverflow.com/questions/61543503/add-legend-to-line-bars-to-altair-chart-without-using-size-color.\n",
    "        scale = alt.Scale(domain=[\"cases\", \"deaths\"], range=['#377eb8', '#e41a1c'])\n",
    "        base = alt.Chart(\n",
    "            cases_and_deaths,\n",
    "            title='(C) Cumulative cases and deaths'\n",
    "        ).transform_calculate(\n",
    "            cases_=\"'cases'\",\n",
    "            deaths_=\"'deaths'\",\n",
    "        )\n",
    "\n",
    "        c = base.mark_line(strokeWidth=3).encode(\n",
    "            x=alt.X(\"Datetime\", axis=alt.Axis(title = 'Date')),\n",
    "            y=alt.Y(\"cases\", axis=alt.Axis(title = 'Count')),\n",
    "            color=alt.Color(\"cases_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "\n",
    "        d = base.mark_line(strokeWidth=3).encode(\n",
    "            x=alt.X(\"Datetime\", axis=alt.Axis(title='Date')),\n",
    "            y=alt.Y(\"deaths\", axis=alt.Axis(title = 'Count')),\n",
    "            color=alt.Color(\"deaths_:N\", scale=scale, title=\"\")\n",
    "        )\n",
    "        with a3:\n",
    "            st.altair_chart(c+d, use_container_width=True)\n",
    "\n",
    "\n",
    "## functions end here, title, sidebar setting and descriptions start here\n",
    "t1, t2 = st.columns(2)\n",
    "with t1:\n",
    "    st.markdown('# COVID-19 Data and Reporting')\n",
    "\n",
    "with t2:\n",
    "    st.write(\"\")\n",
    "    st.write(\"\")\n",
    "    st.write(\"\"\"\n",
    "    **EpiCenter for Disease Dynamics** | School of Veterinary Medicine - UC Davis\n",
    "    \"\"\")\n",
    "\n",
    "st.write(\"\")\n",
    "st.markdown(\"\"\"\n",
    "COVID-Local provides basic key metrics against which to assess pandemic response and progress toward reopening.  \n",
    "Phase 2: Initial re-opening: Current esetimate of <25 cases per 100,000 population per day  \n",
    "Phase 3: Economic recovery: Current estimate of <10 cases per 100,000 population per day   \n",
    "*daily testing data currently available only for Los Angeles County, Orange County, and San Diego County  \n",
    "\n",
    "for more details related to thresholds please see  \n",
    "See more at https://www.covidlocal.org/metrics/.    \n",
    "\n",
    "For additional information please contact *epicenter@ucdavis.edu* or visit https://ohi.vetmed.ucdavis.edu/centers/epicenter-disease-dynamics.  \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "if sidebar_selection == 'Select Counties':\n",
    "    st.markdown('## Select counties of interest')\n",
    "    CA_counties = confirmed[confirmed.Province_State == 'California'].Admin2.unique().tolist()\n",
    "    counties = st.multiselect('', CA_counties, default=['Yolo', 'Solano', 'Sacramento'])\n",
    "    # Limit to the first 5 counties.\n",
    "    counties = counties[:5]\n",
    "    if not counties:\n",
    "        # If no counties are specified, just plot the state.\n",
    "        st.markdown('> No counties were selected, falling back to showing statistics for California state.')\n",
    "        plot_state()\n",
    "    else:\n",
    "        # Plot the aggregate and per-county details.\n",
    "        plot_county(counties)\n",
    "        for c in counties:\n",
    "            st.write('')\n",
    "            with st.expander(f\"Expand for {c} County Details\"):\n",
    "                plot_county([c])\n",
    "elif sidebar_selection == 'California':\n",
    "    plot_state()\n",
    "\n",
    "with st.sidebar.expander(\"Click to learn more about this dashboard\"):\n",
    "    st.markdown(f\"\"\"\n",
    "    One of the key metrics for which data are widely available is the estimate of **daily new cases per 100,000\n",
    "    population**.\n",
    "\n",
    "    Here, in following graphics, we will track:\n",
    "\n",
    "    (A) Estimates of daily new cases per 100,000 population (averaged over the last seven days)  \n",
    "    \n",
    "    (B) Daily incidence (new cases)  \n",
    "    \n",
    "    (C) Cumulative cases and deaths  \n",
    "    \n",
    "    (D) Daily new tests*  \n",
    "\n",
    "    Data source: Data for cases are procured automatically from **COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University**.  \n",
    "    \n",
    "    The data is updated at least once a day or sometimes twice a day in the [COVID-19 Data Repository](https://github.com/CSSEGISandData/COVID-19).  \n",
    "\n",
    "    Infection rate, positive test rate, ICU headroom and contacts traced from https://covidactnow.org/.  \n",
    "\n",
    "    *Calculation of % positive tests depends on consistent reporting of county-wise total number of tests performed routinely. Rolling averages and proportions are not calculated if reporting is inconsistent over a period of 14 days.  \n",
    "\n",
    "    *Report updated on {str(today)}.*  \n",
    "    \"\"\")\n",
    "\n",
    "if _ENABLE_PROFILING:\n",
    "    pr.disable()\n",
    "    s = io.StringIO()\n",
    "    sortby = SortKey.CUMULATIVE\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "    ps.print_stats()\n",
    "    ts = int(time.time())\n",
    "    with open(f\"perf_{ts}.txt\", \"w\") as f:\n",
    "        f.write(s.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "!    streamlit run C:\\Users\\sanke\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae55ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
